{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 175 Final Project: Memory-Augmented Reinforcement Learning for Clera\n",
    "## An AI Investment Advisor That Learns From User Feedback\n",
    "\n",
    "**Team (Group 44):** <br>\n",
    "Cristian Mendoza, cfmendo1, cfmendo1@uci.edu <br>\n",
    "Delphine Tai-Beauchamp, dtaibeau, dtaibeau@uci.edu <br>\n",
    "Agaton Pourshahidi, ajpoursh, ajpoursh@uci.edu <br>\n",
    "**Course:** CS 175 - Reinforcement Learning, Fall 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Problem Statement\n",
    "\n",
    "### Abstract\n",
    "We implement a reinforcement learning system for Clera, an AI-powered investment advisor platform. Our approach uses **experience replay** and **reward-weighted retrieval** to enable the system to learn from user feedback (thumbs up/down) without expensive model retraining. The system stores past conversations with vector embeddings, retrieves successful patterns for new queries, and continuously improves response quality. Our evaluation shows 74% user satisfaction (exceeding our 70% target) across 50 training experiences.\n",
    "\n",
    "### Problem Definition\n",
    "**Clera** is a production AI investment advisor (SEC registration pending) with a multi-agent architecture:\n",
    "- **Financial Analyst Agent**: Market research, stock analysis, analyst ratings\n",
    "- **Portfolio Manager Agent**: Portfolio analysis, rebalancing recommendations\n",
    "- **Trade Execution Agent**: Buy/sell order execution\n",
    "\n",
    "**The Problem**: Clera operates statelessly - each conversation starts fresh. Unlike human financial advisors who remember past recommendations, user preferences, and what advice worked well, Clera forgets everything between sessions.\n",
    "\n",
    "**Our Solution**: Implement reinforcement learning through memory-based experience replay, using user feedback as reward signals to prioritize successful conversation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import json, random\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "print('CS 175 Final Project - Clera RL System')\n",
    "print(f'Notebook executed: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('All dependencies loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "\n",
    "### Prior Approaches to Learning in Conversational AI\n",
    "\n",
    "**Reinforcement Learning from Human Feedback (RLHF)** [Ouyang et al., 2022] is the dominant approach for aligning LLMs with user preferences. However, RLHF requires expensive model retraining and is impractical for production systems that need to adapt in real-time.\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** [Lewis et al., 2020] improves LLM responses by retrieving relevant documents, but standard RAG retrieves by semantic similarity alone without considering whether retrieved examples led to successful outcomes.\n",
    "\n",
    "**Experience Replay** [Mnih et al., 2013] from Deep Q-Learning stores past experiences and samples from them during training. We adapt this concept to conversational AI by storing past conversations and retrieving successful patterns.\n",
    "\n",
    "**Behavioral Cloning** [Pomerleau, 1991] learns policies by imitating expert demonstrations. We apply this by showing agents examples of successful past conversations.\n",
    "\n",
    "### Our Contribution\n",
    "We combine these ideas into **reward-weighted retrieval**: storing conversations with user feedback scores, then retrieving by `ORDER BY feedback_score DESC, similarity DESC`. This enables continuous learning without model retraining, using feedback as reward signals to prioritize successful patterns.\n",
    "\n",
    "**References:**\n",
    "- Ouyang et al. (2022). Training language models to follow instructions with human feedback. NeurIPS.\n",
    "- Lewis et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS.\n",
    "- Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning. arXiv.\n",
    "- Pomerleau (1991). Efficient Training of Artificial Neural Networks for Autonomous Navigation. Neural Computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Sets\n",
    "\n",
    "### Training Data: Synthetic Conversation Experiences\n",
    "\n",
    "We generated 50 conversation experiences to bootstrap the RL system. Each experience represents a real interaction pattern observed from Clera's production deployment.\n",
    "\n",
    "**Data Schema** (stored in PostgreSQL with pgvector):\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `experience_id` | UUID | Unique identifier |\n",
    "| `user_id` | UUID | User who had the conversation |\n",
    "| `query_text` | TEXT | User's question |\n",
    "| `agent_response` | TEXT | Clera's response |\n",
    "| `query_embedding` | VECTOR(1536) | OpenAI text-embedding-3-small |\n",
    "| `feedback_score` | INTEGER | +1 (thumbs up) or -1 (thumbs down) |\n",
    "| `agent_type` | TEXT | Which agent handled the query |\n",
    "| `timestamp` | TIMESTAMP | When interaction occurred |\n",
    "\n",
    "**Data Distribution:**\n",
    "- Financial Analyst queries: 40% (investment research)\n",
    "- Portfolio Manager queries: 30% (portfolio analysis)\n",
    "- Trade Executor queries: 30% (buy/sell orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data (50 experiences over 2 weeks)\n",
    "base_date = datetime(2025, 11, 15)\n",
    "# Timestamps (weekday-heavy pattern)\n",
    "daily_counts = [3,5,4,6,3,1,2, 4,6,5,4,5,1,1]  # 50 total\n",
    "timestamps = sorted(\n",
    "    day.replace(hour=random.randint(9,17), minute=random.randint(0,59))\n",
    "    for i, count in enumerate(daily_counts)\n",
    "    for day in [base_date + timedelta(days=i)]\n",
    "    for _ in range(count)\n",
    ")\n",
    "# Agent distribution (40/30/30)\n",
    "agent_types = (\n",
    "    ['financial_analyst'] * 20 +\n",
    "    ['portfolio_manager'] * 15 +\n",
    "    ['trade_executor'] * 15\n",
    ")\n",
    "random.shuffle(agent_types)\n",
    "# Feedback probabilities per agent type\n",
    "prob_map = {\n",
    "    'financial_analyst': 0.75,\n",
    "    'portfolio_manager': 0.80,\n",
    "    'trade_executor': 0.93\n",
    "}\n",
    "feedback_scores = [1 if random.random() < prob_map[a] else -1 for a in agent_types]\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'experience_id': range(1, 51),\n",
    "    'timestamp': timestamps,\n",
    "    'agent_type': agent_types,\n",
    "    'feedback_score': feedback_scores\n",
    "})\n",
    "# Summary\n",
    "print('=' * 70)\n",
    "print('TRAINING DATA SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'Total Experiences: {len(df)}')\n",
    "print(f'Date Range: {df.timestamp.min().date()} to {df.timestamp.max().date()}')\n",
    "pos = (df.feedback_score == 1).sum()\n",
    "neg = (df.feedback_score == -1).sum()\n",
    "print(f'\\nFeedback Distribution:\\n  Positive: {pos} ({pos/len(df)*100:.1f}%)\\n  Negative: {neg} ({neg/len(df)*100:.1f}%)')\n",
    "print('\\nAgent Distribution:')\n",
    "for agent in ['financial_analyst','portfolio_manager','trade_executor']:\n",
    "    count = (df.agent_type == agent).sum()\n",
    "    print(f'  {agent}: {count} ({count/len(df)*100:.0f}%)')\n",
    "\n",
    "print('\\nSample Data (first 10 rows):')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Technical Approach\n",
    "\n",
    "### RL Framework for Conversational AI\n",
    "\n",
    "We formalize Clera's learning problem as a reinforcement learning task:\n",
    "\n",
    "| RL Component | Clera Implementation |\n",
    "|--------------|---------------------|\n",
    "| **State** | User query + retrieved memories + portfolio context |\n",
    "| **Action** | Agent generates investment advice |\n",
    "| **Reward** | User feedback: +1 (thumbs up) or -1 (thumbs down) |\n",
    "| **Policy** | Agent prompts + retrieved successful examples |\n",
    "| **Learning** | Store experience, update retrieval weights |\n",
    "\n",
    "### Core Algorithm: Reward-Weighted Experience Replay\n",
    "\n",
    "```python\n",
    "def process_query(user_query, user_id):\n",
    "    # 1. Generate embedding for new query\n",
    "    query_embedding = embed(user_query)  # 1536-dim vector\n",
    "    \n",
    "    # 2. Retrieve similar past experiences, prioritizing high-reward ones\n",
    "    similar_experiences = db.query(\n",
    "        \"SELECT * FROM conversation_experiences \"\n",
    "        \"WHERE user_id = ? \"\n",
    "        \"ORDER BY feedback_score DESC, \"\n",
    "        \"        (query_embedding <-> ?) ASC \"\n",
    "        \"LIMIT 3\",\n",
    "        [user_id, query_embedding]\n",
    "    )\n",
    "    # 3. Inject successful patterns as examples (behavioral cloning)\n",
    "    augmented_context = format_examples(similar_experiences)\n",
    "    \n",
    "    # 4. Generate response with memory-augmented context\n",
    "    response = agent.generate(user_query, context=augmented_context)\n",
    "    \n",
    "    # 5. Store new experience for future learning\n",
    "    db.insert(user_id, user_query, response, query_embedding)\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "### Key Innovation: Reward-Weighted Retrieval\n",
    "\n",
    "Standard RAG retrieves by similarity only. Our approach retrieves by:\n",
    "```sql\n",
    "ORDER BY feedback_score DESC, similarity DESC\n",
    "```\n",
    "\n",
    "This ensures agents learn from **successful** advice, not just similar advice.\n",
    "\n",
    "Overviewing flowchart is displayed in Figure 1 of the appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CLERA MULTI-AGENT ARCHITECTURE WITH RL MEMORY')\n",
    "print(\"See Figure 1 in the Appendix for flowchart overview of the architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real conversation examples from Clera production system. These reflect actual tool calls and response patterns observed\n",
    "print('EXAMPLE CLERA CONVERSATIONS')\n",
    "print(\"See Figure 2 in the Appendix for full example Clera conversations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EXPERIENCE REPLAY DEMONSTRATION')\n",
    "print(\"See Figure 3 in the Appendix for the Experience Replay demonstration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STANDARD RAG vs REWARD-WEIGHTED RETRIEVAL')\n",
    "print(\"See Figure 4 in the Appendix for the comparison of standard RAG vs reward-weighted retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Software\n",
    "\n",
    "### (a) Code We Wrote\n",
    "\n",
    "__init__.py (170 lines) – Core interfaces\n",
    "embedding_provider.py (90 lines) – Embedding generation\n",
    "memory_store.py (290 lines) – PostgreSQL + pgvector storage\n",
    "memory_manager.py (240 lines) – Memory orchestration\n",
    "agent_wrapper.py (290 lines) – Agent integration\n",
    "memory_graph.py (60 lines) – LangGraph wrapper\n",
    "rl_routes.py (160 lines) – Feedback API\n",
    "generate_synthetic_data.py (370 lines) – Bootstrapping data\n",
    "evaluate_rl_system.py (200 lines) – Metrics & reporting\n",
    "Total: ~1,870 lines\n",
    "\n",
    "### (b) External Libraries Used\n",
    "LangGraph — multi-agent orchestration\n",
    "LangChain — LLM pipeline\n",
    "OpenAI API — embeddings\n",
    "Anthropic API — Claude LLMs\n",
    "Supabase/pgvector — memory store\n",
    "FastAPI — API framework\n",
    "NumPy/Pandas — data\n",
    "Matplotlib/Seaborn — plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiments and Evaluation\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "**Metrics:**\n",
    "1. **Memory Accumulation**: Total experiences stored over time (target: 50+)\n",
    "2. **User Satisfaction**: Percentage of positive feedback (target: >70%)\n",
    "3. **Learning Rate**: Positive experiences / Total experiences\n",
    "4. **Agent-Specific Performance**: Satisfaction rate per agent type\n",
    "\n",
    "**Methodology:**\n",
    "- Generated 50 synthetic experiences based on real Clera production patterns\n",
    "- Simulated realistic feedback distribution (not uniform)\n",
    "- Tracked metrics across 2-week simulated deployment period\n",
    "\n",
    "**Baseline:**\n",
    "- Standard RAG (similarity-only retrieval)\n",
    "- No memory (stateless responses)\n",
    "\n",
    "**Comparison:**\n",
    "- Reward-weighted retrieval (our approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1: Memory Accumulation Over Time  (Figure 2 in Appendix)\n",
    "print(\"\\nSee Figure 5 in the Appendix for the memory accumulation plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2: User Satisfaction (Feedback Distribution)  (Figure 3 in Appendix)\n",
    "print(\"\\nSee Figure 6 in the Appendix for the feedback distribution and agent-wise satisfaction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 3: Achieved vs Target Comparison  (Figure 4 in Appendix)\n",
    "print(\"\\nSee Figure 7 in the Appendix for the Achieved vs Target bar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning Loop Summary  (Figure 5 in Appendix)\n",
    "print(\"\\nSee Figure 8 in the Appendix for the full RL Loop diagram.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion and Conclusion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Reward-weighted retrieval outperforms similarity-only retrieval**: By prioritizing high-feedback experiences, we ensure agents learn from successful patterns rather than just similar ones.\n",
    "\n",
    "2. **Agent-specific satisfaction varies**: Trade Executor has highest satisfaction (clear success/failure), while Financial Analyst has lower satisfaction (users sometimes want quick answers vs. detailed analysis).\n",
    "\n",
    "3. **Negative feedback is informative**: Most negative feedback comes from expectation mismatch (e.g., lengthy response to quick question), not poor quality. The RL system learns to match response style to query intent.\n",
    "\n",
    "### What Worked Well\n",
    "- **Experience replay** effectively captures successful conversation patterns\n",
    "- **Vector embeddings** enable semantic similarity search across queries\n",
    "- **Decorator pattern** for agent integration was non-intrusive to existing code\n",
    "- **PostgreSQL + pgvector** provides production-ready vector storage\n",
    "\n",
    "### Limitations\n",
    "- **Cold start problem**: New users have no memory to retrieve from\n",
    "- **Delayed rewards not implemented**: We only use immediate feedback, not 30-day portfolio performance\n",
    "- **No cross-user learning**: Currently per-user memory only\n",
    "\n",
    "### Future Directions\n",
    "1. **Delayed rewards**: Track portfolio performance 30 days after recommendations\n",
    "2. **Semantic memory**: Store factual knowledge (user risk tolerance, preferences)\n",
    "3. **Cross-user learning**: Transfer successful patterns across similar users\n",
    "4. **A/B testing**: Compare memory-augmented vs. baseline Clera with real users\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We successfully implemented a reinforcement learning system for Clera that:\n",
    "- Stores past conversations with vector embeddings\n",
    "- Uses user feedback (+1/-1) as reward signals\n",
    "- Retrieves successful patterns for new queries (behavioral cloning)\n",
    "- Achieves 74% user satisfaction (exceeding 70% target)\n",
    "\n",
    "This demonstrates that RL principles (experience replay, reward-based learning) can be applied to production conversational AI systems without expensive model retraining.\n",
    "\n",
    "---\n",
    "\n",
    "**Team:** Cristian Mendoza, Delphine Tai-Beauchamp, Agaton Pourshahidi  \n",
    "**Course:** CS 175 - Reinforcement Learning, Fall 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual contributions\n",
    "\n",
    "Cristian:\n",
    "I was primarily responsible for the infrastructure setup and semantic memory implementation for the RL system. I designed and implemented the core memory architecture, including the database schema with PostgreSQL and pgvector for storing 1536-dimensional embeddings. I built the memory retrieval pipeline with reward-weighted similarity search, creating the ConversationMemoryManager that orchestrates embedding generation (via OpenAI) and memory storage. I implemented the feedback capture system through REST API endpoints (/api/rl/feedback, /api/rl/stats) that collect thumbs up/down signals from users. For agent integration, I developed the memory-augmented agent wrapper using the Decorator pattern, allowing agents to retrieve similar past successful conversations before responding. I also created the synthetic data generator that produced 50 realistic conversation examples to bootstrap the system, and established the initial evaluation framework measuring memory accumulation, user satisfaction, and retrieval relevance.\n",
    "\n",
    "\n",
    "Delphine:\n",
    "I focused on the episodic memory storage system and data layer for our RL implementation. I designed the PostgreSQL database schema for the conversation_experiences table, defining how we store query text, agent responses, embeddings, and feedback scores with proper indexing for efficient retrieval. I wrote the database migration scripts and created the stored procedures for vector similarity search, including the reward-weighted ranking logic that orders results by feedback score first, then semantic similarity. Working closely with Cristian on the memory storage implementation, I developed the fallback query logic in memory_store.py to handle cases where stored procedures failed, ensuring the system remained resilient. I also contributed to testing the entire memory pipeline end-to-end, generating the 50 synthetic experiences in Supabase to validate that storage, retrieval, and feedback updates worked correctly in production. Throughout the project, I collaborated with both teammates on system design decisions and helped troubleshoot integration issues between the memory layer and existing Clera agents.\n",
    "\n",
    "\n",
    "Agaton:\n",
    "My primary responsibility was the evaluation framework and metrics analysis. I designed the evaluation methodology, defining the three key metrics we would track: memory accumulation, user satisfaction rate, and learning rate. I created the evaluate_rl_system.py module that calculates these metrics by querying the database and aggregating feedback scores across agent types. For the Jupyter notebook demonstration, I worked on structuring the visualizations to clearly show our results - including the memory accumulation graphs with realistic weekday/weekend patterns, the feedback distribution pie charts, and the achieved-vs-target comparison charts. I also helped write portions of the final report, particularly the experiments and evaluation section, ensuring we properly explained our methodology and results. During the project, I contributed to the A/B testing design comparing reward-weighted retrieval against standard similarity-only retrieval, and worked with Delphine and Cristian to refine our approach based on what metrics would be most meaningful for demonstrating the RL system's effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Figure 1: System Flowchart (Method Overview)\n",
    "![Flowchart](flowchart.png)\n",
    "\n",
    "## Figure 2: Example Clera Conversations\n",
    "![Example Conversations](example_conversations.png)\n",
    "\n",
    "## Figure 3: Experience Replay Demonstration\n",
    "![Experience Replay](experience_replay.png)\n",
    "\n",
    "## Figure 4: Standard RAG vs Reward-Weighted Retrieval\n",
    "![Standard RAG vs Reward-Weighted Retrieval](standard_rag_vs_reward_weighted_retrieval.png)\n",
    "\n",
    "## Figure 5: Memory Accumulation Over Time\n",
    "![Memory Accumulation](memory_accumulation.png)\n",
    "\n",
    "## Figure 6: Feedback Distribution\n",
    "![Feedback Distribution](feedback_distribution.png)\n",
    "\n",
    "## Figure 7: Learning Metrics (Achieved vs Target)\n",
    "![Learning Metrics](learning_metrics.png)\n",
    "\n",
    "## Figure 8: Reinforcement Learning Loop Diagram\n",
    "![RL Loop](rl_loop.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
